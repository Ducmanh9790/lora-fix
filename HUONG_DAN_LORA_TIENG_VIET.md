# ğŸš€ HÆ°á»›ng Dáº«n LoRA - Tutorial HoÃ n Chá»‰nh (Tiáº¿ng Viá»‡t)

## ğŸ“Œ Tá»•ng Quan

Repository nÃ y chá»©a má»™t **triá»ƒn khai LoRA (Low-Rank Adaptation) hoÃ n chá»‰nh** cho cáº£ **NLG (Sinh VÄƒn Báº£n)** vÃ  **NLU (PhÃ¢n Loáº¡i VÄƒn Báº£n)** vá»›i cÃ¡c hÆ°á»›ng dáº«n, vÃ­ dá»¥ vÃ  so sÃ¡nh chi tiáº¿t.

**LoRA lÃ  gÃ¬?** Má»™t phÆ°Æ¡ng phÃ¡p fine-tuning tiáº¿t kiá»‡m tham sá»‘ cho phÃ©p:
- âœ… Giáº£m tham sá»‘ trainable Ä‘áº¿n **98%+**
- âœ… Tiáº¿t kiá»‡m **99%+ dung lÆ°á»£ng** (4-6 MB thay vÃ¬ 330-475 MB)
- âœ… TÄƒng tá»‘c Ä‘á»™ training **2-6x**
- âœ… Äáº¡t **95-98% cháº¥t lÆ°á»£ng** so vá»›i full fine-tuning
- âœ… Cho phÃ©p **há»c Ä‘a tÃ¡c vá»¥** trÃªn má»™t GPU

---

## ğŸ—‚ï¸ Cáº¥u TrÃºc Repository

```
lora/
â”œâ”€â”€ loralib/                    # Triá»ƒn khai LoRA cá»‘t lÃµi
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ layers.py              # Lá»›p linear vá»›i LoRA
â”‚   â””â”€â”€ utils.py               # HÃ m há»— trá»£
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ NLG/                   # Sinh vÄƒn báº£n (GPT-2)
â”‚   â”‚   â”œâ”€â”€ run_training.py              # Train trÃªn dataset E2E
â”‚   â”‚   â”œâ”€â”€ run_inference.py             # Sinh vÄƒn báº£n tá»« checkpoint
â”‚   â”‚   â”œâ”€â”€ compare_lora_vs_full.py     # So sÃ¡nh hiá»‡u quáº£
â”‚   â”‚   â”œâ”€â”€ evaluate_lora_improvement.py # Metrics cháº¥t lÆ°á»£ng
â”‚   â”‚   â””â”€â”€ data/                        # Dataset E2E NLG
â”‚   â”‚
â”‚   â”œâ”€â”€ NLU/                   # PhÃ¢n loáº¡i vÄƒn báº£n (RoBERTa)
â”‚   â”‚   â”œâ”€â”€ run_training_nlu.py          # Train trÃªn GLUE tasks
â”‚   â”‚   â”œâ”€â”€ run_inference_nlu.py         # PhÃ¢n loáº¡i vÄƒn báº£n
â”‚   â”‚   â”œâ”€â”€ benchmark_multi_task.py      # Benchmark Ä‘a tÃ¡c vá»¥
â”‚   â”‚   â”œâ”€â”€ evaluate_lora_improvement.py # Metrics cháº¥t lÆ°á»£ng
â”‚   â”‚   â””â”€â”€ data/                        # Datasets GLUE
â”‚   â”‚
â”‚   â”œâ”€â”€ QUALITY_COMPARISON.py            # So sÃ¡nh cháº¥t lÆ°á»£ng chi tiáº¿t
â”‚   â”œâ”€â”€ COMPARISON_RESULTS.py            # Baseline tá»« pretrained
â”‚   â”œâ”€â”€ VISUAL_COMPARISON.py             # Biá»ƒu Ä‘á»“ & báº£ng
â”‚   â”œâ”€â”€ LORA_vs_FULL_COMPARISON.py       # PhÃ¢n tÃ­ch chi phÃ­
â”‚   â”‚
â”‚   â””â”€â”€ TÃ i liá»‡u/
â”‚       â”œâ”€â”€ 00_START_HERE.md             # Quick start cho NLG
â”‚       â”œâ”€â”€ COMPARISON_GUIDE.md          # HÆ°á»›ng dáº«n so sÃ¡nh
â”‚       â”œâ”€â”€ MODEL_COMPARISON_DETAILED.md # So sÃ¡nh chi tiáº¿t
â”‚       â”œâ”€â”€ QUALITY_COMPARISON_SUMMARY.md # PhÃ¢n tÃ­ch chÃªnh lá»‡ch
â”‚       â””â”€â”€ NLU_GUIDE.md                 # HÆ°á»›ng dáº«n NLU
â”‚
â””â”€â”€ README.md                  # File nÃ y

```

---

## ğŸš€ Báº¯t Äáº§u Nhanh (5 PhÃºt)

### 1. CÃ i Äáº·t

```bash
# Clone repository
git clone https://github.com/Ducmanh9790/lora-fix.git
cd lora

# Táº¡o virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# CÃ i Ä‘áº·t packages cáº§n thiáº¿t
pip install torch transformers numpy tqdm scikit-learn
```

### 2. Kiá»ƒm Tra Triá»ƒn Khai LoRA

```bash
# Xem cÃ¡c class LoRA cá»‘t lÃµi
cat loralib/layers.py

# CÃ¡c class chÃ­nh:
# - Linear: Lá»›p linear cÃ³ LoRA adaptation
# - mark_only_lora_as_trainable(): Freeze params khÃ´ng pháº£i LoRA
# - lora_state_dict(): LÆ°u chá»‰ weights LoRA
```

### 3. Demo Nhanh

```bash
# Test NLG (Sinh VÄƒn Báº£n)
cd examples/NLG
python run_training.py      # Train 2 epochs
python run_inference.py     # Sinh vÄƒn báº£n

# Test NLU (PhÃ¢n Loáº¡i)
cd ../NLU
python run_training_nlu.py  # Train trÃªn SST-2
python run_inference_nlu.py # PhÃ¢n loáº¡i vÄƒn báº£n
```

---

## ğŸ“š Lá»™ TrÃ¬nh Há»c Táº­p HoÃ n Chá»‰nh

### BÆ°á»›c 1: Hiá»ƒu CÆ¡ Báº£n LoRA (10 phÃºt)

```bash
cat examples/00_START_HERE.md
```

**CÃ¡c KhÃ¡i Niá»‡m ChÃ­nh:**
- LoRA = Low-Rank Adaptation (CÃ¡ch Tiáº¿p Cáº­n Rank Tháº¥p)
- Ã tÆ°á»Ÿng: A = U @ V^T (oÃ¹ U, V lÃ  ma tráº­n rank tháº¥p)
- Original layer: output = Wx + b
- With LoRA: output = Wx + (Î±/r) Ã— B(Ax) + b
- Lá»£i Ã­ch: Chá»‰ train A vÃ  B (1-2% tham sá»‘)

### BÆ°á»›c 2: Triá»ƒn Khai NLG (30 phÃºt)

**File: `examples/NLG/run_training.py`**

```bash
cd examples/NLG
python run_training.py
```

**Äiá»u gÃ¬ xáº£y ra:**
1. Load pretrained GPT-2 (124M params)
2. ThÃªm LoRA adapters vÃ o attention layers
3. Freeze 98% tham sá»‘
4. Train trÃªn dataset E2E
5. LÆ°u checkpoint 4 MB

**Output:**
```
Training completed!
Model statistics:
  Total params: 124,439,808
  Trainable params: 1,060,480 (0.85%)
  Checkpoint saved: 4.06 MB
```

**Äoáº¡n Code ChÃ­nh:**
```python
import loralib as lora

# 1. Load model
model = GPT2LMHeadModel.from_pretrained('gpt2')

# 2. ÄÃ¡nh dáº¥u chá»‰ LoRA params lÃ  trainable
lora.mark_only_lora_as_trainable(model)

# 3. Train (chá»‰ LoRA layers Ä‘Æ°á»£c update)
optimizer.step()

# 4. Save (chá»‰ LoRA weights)
checkpoint = {k: v for k, v in model.state_dict().items() if 'lora' in k}
torch.save(checkpoint, 'checkpoint.bin')
```

### BÆ°á»›c 3: Triá»ƒn Khai NLU (30 phÃºt)

**File: `examples/NLU/run_training_nlu.py`**

```bash
cd examples/NLU
python run_training_nlu.py
```

**Há»— Trá»£ 8 GLUE Tasks:**
- **SST-2**: PhÃ¢n tÃ­ch cáº£m xÃºc
- **MNLI**: PhÃ¢n loáº¡i kÃ©o theo
- **QNLI**: Tráº£ lá»i cÃ¢u há»i
- **RTE**: KÃ©o theo vÄƒn báº£n
- **MRPC**: Äá»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a
- **CoLA**: TÃ­nh ngá»¯ phÃ¡p
- **QQP**: PhÃ¡t hiá»‡n paraphrase
- **STS-B**: Äá»™ tÆ°Æ¡ng tá»± vÄƒn báº£n ngá»¯ nghÄ©a

**Output:**
```
Training SST-2 task completed!
Model statistics:
  Total params: 124,647,170
  Trainable params: 1,470,464 (1.18%)
  Checkpoint saved: 5.64 MB
```

### BÆ°á»›c 4: ÄÃ¡nh GiÃ¡ & So SÃ¡nh (20 phÃºt)

```bash
cd examples

# 1. Xem baseline tá»« pretrained
python COMPARISON_RESULTS.py

# 2. Biá»ƒu Ä‘á»“ metrics
python VISUAL_COMPARISON.py

# 3. So sÃ¡nh cháº¥t lÆ°á»£ng chi tiáº¿t
python QUALITY_COMPARISON.py

# 4. LoRA vs Full efficiency
python LORA_vs_FULL_COMPARISON.py
```

---

## ğŸ¯ Hiá»ƒu Káº¿t Quáº£

### So SÃ¡nh Cháº¥t LÆ°á»£ng

| Task | Pretrained | Full FT | LoRA | ChÃªnh |
|------|-----------|---------|------|-------|
| **NLU Accuracy** | 60% | 95% | 93.5% | -1.5pp |
| **NLG BLEU** | ~32 | ~45 | ~43 | -2 (-4.4%) |
| **Dung lÆ°á»£ng** | - | 475 MB | 4-6 MB | **99% nhá» hÆ¡n** |
| **Thá»i gian train** | - | 6 giá» | 1.5 giá» | **4x nhanh hÆ¡n** |

**Káº¿t Luáº­n:** LoRA Ä‘áº¡t 95-98% cháº¥t lÆ°á»£ng vá»›i dung lÆ°á»£ng 99% nhá» hÆ¡n vÃ  training 4x nhanh hÆ¡n!

### Hiá»‡u Quáº£ Tham Sá»‘

```
Full Fine-tuning:
  Tá»•ng: 124M params
  Trainable: 124M params
  Frozen: 0%

LoRA Fine-tuning:
  Tá»•ng: 124M params
  Trainable: 1.2-1.5M params (0.85-1.2%)
  Frozen: 98-99%

Lá»£i Ã­ch: Ãt overfitting, generalization tá»‘t hÆ¡n
```

---

## ğŸ“– HÆ°á»›ng Dáº«n Äá»c

### Äá»ƒ Hiá»ƒu Nhanh (15 phÃºt):
1. `examples/00_START_HERE.md` - Giá»›i thiá»‡u nhanh
2. `examples/COMPARISON_GUIDE.md` - File nÃ o cháº¡y
3. File nÃ y

### Äá»ƒ Há»c Chi Tiáº¿t (1-2 giá»):
1. `examples/MODEL_COMPARISON_DETAILED.md` - So sÃ¡nh Ä‘áº§y Ä‘á»§
2. `examples/QUALITY_COMPARISON_SUMMARY.md` - PhÃ¢n tÃ­ch chÃªnh lá»‡ch
3. `examples/NLU_GUIDE.md` - HÆ°á»›ng dáº«n NLU
4. Äá»c cÃ¡c script Python

### Äá»ƒ Triá»ƒn Khai (Theo dÃµi code):
1. `examples/NLG/run_training.py` - Há»c code
2. `examples/NLU/run_training_nlu.py` - Há»c code
3. Sá»­a Ä‘á»•i vÃ  thá»­ nghiá»‡m

---

## ğŸ”§ CÃ¡ch Sá»­ Dá»¥ng (CÃ¡c TÃ¬nh Huá»‘ng ThÃ´ng ThÆ°á»ng)

### TÃ¬nh Huá»‘ng 1: Fine-tune TrÃªn Dá»¯ Liá»‡u Cá»§a Báº¡n

```python
import loralib as lora
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# 1. Load model
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 2. ThÃªm LoRA adapters
lora.mark_only_lora_as_trainable(model)

# 3. Chuáº©n bá»‹ dá»¯ liá»‡u
texts = ["vÄƒn báº£n cá»§a báº¡n 1", "vÄƒn báº£n cá»§a báº¡n 2", ...]
inputs = tokenizer(texts, return_tensors='pt', max_length=512, truncation=True)

# 4. Train
model.train()
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

for epoch in range(3):
    for batch in dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# 5. Save checkpoint (chá»‰ LoRA weights)
torch.save(model.state_dict(), 'checkpoint.bin')
```

### TÃ¬nh Huá»‘ng 2: Load Checkpoint & Suy Luáº­n

```python
# 1. Load base model
model = GPT2LMHeadModel.from_pretrained('gpt2')
lora.mark_only_lora_as_trainable(model)

# 2. Load checkpoint
checkpoint = torch.load('checkpoint.bin')
model.load_state_dict(checkpoint, strict=False)

# 3. Suy luáº­n
model.eval()
with torch.no_grad():
    input_ids = tokenizer.encode("Sinh vÄƒn báº£n:", return_tensors='pt')
    output = model.generate(input_ids, max_length=50)
    text = tokenizer.decode(output[0])
```

### TÃ¬nh Huá»‘ng 3: Há»c Äa TÃ¡c Vá»¥

```bash
# Train 5 models trÃªn cÃ¡c tÃ¡c vá»¥ khÃ¡c nhau
cd examples/NLU

# Train tá»«ng tÃ¡c vá»¥
python -c "
from run_training_nlu import train_model
for task in ['sst2', 'mnli', 'qnli', 'rte', 'mrpc']:
    train_model(task)
    # Má»—i cÃ¡i lÆ°u 5-6 MB checkpoint
"

# Tá»•ng dung lÆ°á»£ng: 25-30 MB (vs 1.9 GB cho full fine-tuning!)
```

---

## ğŸ“Š TÃ i Liá»‡u Tham Kháº£o Metrics

### Hiá»‡u Suáº¥t Training
- **Tá»‘c Ä‘á»™**: 2-6x nhanh hÆ¡n (Ã­t gradients tÃ­nh toÃ¡n)
- **Bá»™ nhá»›**: 3-6x Ã­t hÆ¡n (Ã­t params backprop)
- **Batch Size**: 4-8x lá»›n hÆ¡n (bá»™ nhá»› tháº¥p hÆ¡n)
- **GPU**: Hoáº¡t Ä‘á»™ng trÃªn GPU <8GB VRAM

### Metrics Cháº¥t LÆ°á»£ng
- **Accuracy Gap**: 1-2% (cÃ³ thá»ƒ bá» qua cho háº§u háº¿t á»©ng dá»¥ng)
- **BLEU Gap**: 2-4 Ä‘iá»ƒm (váº«n cháº¥t lÆ°á»£ng cao)
- **F1 Score Gap**: 0.01-0.03 (khÃ´ng Ä‘Ã¡ng ká»ƒ)
- **ÄÃ¡nh giÃ¡ Con NgÆ°á»i**: KhÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c vá»›i full FT

### Dung LÆ°á»£ng
- **Checkpoint**: 4-6 MB vs 330-475 MB
- **Giáº£m**: 99%+ nhá» hÆ¡n
- **Quy mÃ´**: CÃ³ thá»ƒ lÆ°u 50+ models trong 1 GB

---

## âœ¨ So SÃ¡nh PhÆ°Æ¡ng PhÃ¡p

### Full Fine-tuning
```
Æ¯u Ä‘iá»ƒm:
  âœ“ Äá»™ chÃ­nh xÃ¡c tá»‘i Ä‘a (100%)
  âœ“ TÃ­nh linh hoáº¡t tá»‘i Ä‘a
  âœ“ PhÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c cÃ´ng nháº­n

NhÆ°á»£c Ä‘iá»ƒm:
  âœ— Chi phÃ­ cao ($600 cho 10 models)
  âœ— Training cháº­m (6 giá»/tÃ¡c vá»¥)
  âœ— Dung lÆ°á»£ng lá»›n (23 GB cho 50 models)
  âœ— Rá»§i ro overfitting trÃªn dá»¯ liá»‡u nhá»
```

### LoRA Fine-tuning
```
Æ¯u Ä‘iá»ƒm:
  âœ“ Chi phÃ­ hiá»‡u quáº£ (tiáº¿t kiá»‡m 75%)
  âœ“ Training nhanh (1.5 giá»/tÃ¡c vá»¥)
  âœ“ Dung lÆ°á»£ng nhá» gá»n (giáº£m 99%)
  âœ“ Generalization tá»‘t hÆ¡n
  âœ“ Kháº£ nÄƒng há»c Ä‘a tÃ¡c vá»¥
  âœ“ 95-98% cháº¥t lÆ°á»£ng

NhÆ°á»£c Ä‘iá»ƒm:
  âœ— Äá»™ chÃ­nh xÃ¡c tháº¥p hÆ¡n (1-2%)
  âœ— Linh hoáº¡t kÃ©m cho tÃ¹y chá»‰nh cá»±c Ä‘oan
```

---

## ğŸ¯ Ma Tráº­n Quyáº¿t Äá»‹nh

| TÃ¬nh Huá»‘ng | Khuyáº¿n Nghá»‹ | LÃ½ Do |
|-----------|---|---|
| **1-2 tÃ¡c vá»¥ quan trá»ng** | Full | 1-2% tá»‘t hÆ¡n Ä‘Ã¡ng giÃ¡ |
| **Nhiá»u tÃ¡c vá»¥ (2+)** | **LoRA** | Má»Ÿ rá»™ng tá»‘t, tiáº¿t kiá»‡m 75% |
| **Budget háº¡n cháº¿** | **LoRA** | Giáº£m 75% chi phÃ­ |
| **Edge deployment** | **LoRA** | 4 MB vs 475 MB |
| **NghiÃªn cá»©u/thá»­ nghiá»‡m** | **LoRA** | Iteration 4x nhanh |
| **Y táº¿/phÃ¡p lÃ½** | Full | An toÃ n tá»‘i quan trá»ng |
| **á»¨ng dá»¥ng thÆ°Æ¡ng máº¡i** | **LoRA** | 98% cháº¥t lÆ°á»£ng xuáº¥t sáº¯c |
| **Platform SaaS** | **LoRA** | Má»Ÿ rá»™ng tá»›i 50+ khÃ¡ch hÃ ng |

---

## ğŸ”— TÃ i Liá»‡u Tham Kháº£o File

### Triá»ƒn Khai Cá»‘t LÃµi
- `loralib/layers.py` - Triá»ƒn khai lá»›p LoRA Linear
- `loralib/utils.py` - HÃ m há»— trá»£

### NLG (Sinh VÄƒn Báº£n)
- `examples/NLG/run_training.py` - Script training
- `examples/NLG/run_inference.py` - Script suy luáº­n
- `examples/NLG/compare_lora_vs_full.py` - So sÃ¡nh hiá»‡u quáº£

### NLU (PhÃ¢n Loáº¡i VÄƒn Báº£n)
- `examples/NLU/run_training_nlu.py` - Training 8 GLUE tasks
- `examples/NLU/run_inference_nlu.py` - PhÃ¢n loáº¡i suy luáº­n
- `examples/NLU/benchmark_multi_task.py` - Benchmark Ä‘a tÃ¡c vá»¥

### PhÃ¢n TÃ­ch & So SÃ¡nh
- `examples/QUALITY_COMPARISON.py` - Metrics cháº¥t lÆ°á»£ng chi tiáº¿t
- `examples/VISUAL_COMPARISON.py` - Biá»ƒu Ä‘á»“ vÃ  báº£ng
- `examples/COMPARISON_RESULTS.py` - Metrics baseline
- `examples/LORA_vs_FULL_COMPARISON.py` - PhÃ¢n tÃ­ch chi phÃ­

### TÃ i Liá»‡u
- `examples/00_START_HERE.md` - HÆ°á»›ng dáº«n nhanh
- `examples/COMPARISON_GUIDE.md` - HÆ°á»›ng dáº«n so sÃ¡nh
- `examples/MODEL_COMPARISON_DETAILED.md` - So sÃ¡nh chi tiáº¿t
- `examples/QUALITY_COMPARISON_SUMMARY.md` - PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng
- `examples/NLU_GUIDE.md` - Tutorial NLU

---

## ğŸ“š TÃ i NguyÃªn Há»c Táº­p

### TÃ i Liá»‡u ChÃ­nh Thá»©c
- **LoRA Paper**: https://arxiv.org/abs/2106.09714
- **Official GitHub**: https://github.com/microsoft/LoRA
- **GLUE Benchmark**: https://gluebenchmark.com/
- **E2E NLG Challenge**: https://www.e2e-dataset.org/

### Thá»© Tá»± Äá»c Khuyáº¿n Nghá»‹
1. LoRA Paper (Abstract + Method) - 10 phÃºt
2. `examples/00_START_HERE.md` - 5 phÃºt
3. `examples/NLU_GUIDE.md` - 20 phÃºt
4. `examples/MODEL_COMPARISON_DETAILED.md` - 30 phÃºt
5. Há»c `run_training.py` - 30 phÃºt

---

## ğŸ“ VÃ­ Dá»¥ Code

### VÃ­ Dá»¥ 1: Training LoRA ÄÆ¡n Giáº£n

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import loralib as lora
import torch

# Load model
model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=2)
tokenizer = AutoTokenizer.from_pretrained('roberta-base')

# ThÃªm LoRA
lora.mark_only_lora_as_trainable(model)

# Äáº¿m tham sá»‘
total = sum(p.numel() for p in model.parameters())
trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Tá»•ng: {total}, Trainable: {trainable} ({100*trainable/total:.2f}%)")
# Output: Tá»•ng: 124647170, Trainable: 1470464 (1.18%)

# Train
model.train()
optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=5e-5)
# ... vÃ²ng láº·p training ...
```

### VÃ­ Dá»¥ 2: Load & Merge Checkpoint

```python
# Load base model vá»›i LoRA
model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=2)
lora.mark_only_lora_as_trainable(model)

# Load checkpoint
checkpoint = torch.load('sst2_checkpoint.bin')
model.load_state_dict(checkpoint, strict=False)

# Merge LoRA vÃ o base weights (suy luáº­n nhanh hÆ¡n)
for name, module in model.named_modules():
    if hasattr(module, 'lora_a'):
        # Merge lora_b @ lora_a vÃ o weight
        module.weight.data += (module.lora_alpha / module.r) * (module.lora_b.weight @ module.lora_a.weight)

# Suy luáº­n
model.eval()
with torch.no_grad():
    inputs = tokenizer("Phim tuyá»‡t vá»i!", return_tensors='pt')
    outputs = model(**inputs)
```

---

## ğŸ› Kháº¯c Phá»¥c Sá»± Cá»‘

### Váº¥n Äá» 1: Lá»—i Háº¿t Bá»™ Nhá»›
```
Giáº£i phÃ¡p: Sá»­ dá»¥ng LoRA vá»›i rank nhá» hÆ¡n (r=8 thay vÃ¬ 16)
hoáº·c giáº£m batch size
```

### Váº¥n Äá» 2: Káº¿t Quáº£ Cháº¥t LÆ°á»£ng KÃ©m
```
Giáº£i phÃ¡p: Train lÃ¢u hÆ¡n (nhiá»u epochs hÆ¡n)
hoáº·c sá»­ dá»¥ng learning rate lá»›n hÆ¡n (1e-4)
```

### Váº¥n Äá» 3: Checkpoint KhÃ´ng Load
```
Giáº£i phÃ¡p: HÃ£y cháº¯c cháº¯n dÃ¹ng strict=False khi load
model.load_state_dict(checkpoint, strict=False)
```

---

## ğŸ“ Lá»‡nh Tham Kháº£o Nhanh

```bash
# CÃ i Ä‘áº·t dependencies
pip install torch transformers numpy tqdm scikit-learn

# Cháº¡y NLG training
cd examples/NLG && python run_training.py

# Cháº¡y NLU training
cd examples/NLU && python run_training_nlu.py

# Xem so sÃ¡nh
cd examples && python QUALITY_COMPARISON.py

# Kiá»ƒm tra cáº¥u trÃºc file
find . -type f -name "*.py" | head -20
```

---

## ğŸ¯ TÃ³m Táº¯t

| KhÃ­a Cáº¡nh | Chi Tiáº¿t |
|---------|---------|
| **LoRA lÃ  gÃ¬?** | PhÆ°Æ¡ng phÃ¡p fine-tuning tiáº¿t kiá»‡m tham sá»‘ |
| **Tiáº¿t kiá»‡m bao nhiÃªu?** | 98% tham sá»‘, 99% dung lÆ°á»£ng, 4x nhanh hÆ¡n |
| **Máº¥t cháº¥t lÆ°á»£ng bao nhiÃªu?** | Chá»‰ 1-2% (95-98% so vá»›i full fine-tuning) |
| **Tá»‘t nháº¥t cho?** | Äa tÃ¡c vá»¥, edge, tá»‘i Æ°u chi phÃ­ |
| **Triá»ƒn Khai** | CÃ³ trong loralib/ |
| **VÃ­ Dá»¥** | NLG (GPT-2) + NLU (RoBERTa) |
| **TÃ i Liá»‡u** | HÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§ + scripts |

---

## âœ… CÃ¡c BÆ°á»›c Tiáº¿p Theo

1. **Äá»c** `examples/00_START_HERE.md` (5 phÃºt)
2. **Cháº¡y** `examples/NLG/run_training.py` (10 phÃºt)
3. **Cháº¡y** `examples/QUALITY_COMPARISON.py` (5 phÃºt)
4. **Há»c** `examples/MODEL_COMPARISON_DETAILED.md` (30 phÃºt)
5. **Triá»ƒn Khai** LoRA fine-tuning cá»§a báº¡n!

---

## ğŸ Ná»™i Dung Repository

### Táº¥t Cáº£ CÃ¡c Script & TÃ i Liá»‡u
- âœ… 6 scripts so sÃ¡nh chi tiáº¿t
- âœ… 10+ tÃ i liá»‡u markdown hoÃ n chá»‰nh
- âœ… VÃ­ dá»¥ NLG vÃ  NLU
- âœ… Benchmarks hiá»‡u suáº¥t
- âœ… HÆ°á»›ng dáº«n cÃ i Ä‘áº·t
- âœ… Code vÃ­ dá»¥ hoÃ n chá»‰nh

### CÃ´ng Cá»¥ & Framework
- PyTorch: Deep learning framework
- Transformers: Pre-trained models
- scikit-learn: Metrics vÃ  ML utilities
- tqdm: Progress bars

---

## ğŸ’¡ Máº¹o & Trik

1. **Báº¯t Ä‘áº§u nhá»**: r=8 trÆ°á»›c, sau Ä‘Ã³ r=16 náº¿u cáº§n
2. **Learning rate**: 5e-5 cho háº§u háº¿t tasks
3. **Epochs**: 2-3 epochs Ä‘á»§ tá»‘t
4. **Batch size**: 8-16 cho GPU 8GB
5. **Merge weights**: LÃ m sau training Ä‘á»ƒ inference nhanh

---

**ChÃºc báº¡n há»c táº­p & triá»ƒn khai LoRA vui váº»! ğŸš€**

Náº¿u cÃ³ cÃ¢u há»i, tham kháº£o paper chÃ­nh thá»©c hoáº·c cÃ¡c file tÃ i liá»‡u.

Cáº­p nháº­t láº§n cuá»‘i: ThÃ¡ng 12, 2024
