#!/usr/bin/env python3
"""
NLG Model Evaluation - So sÃ¡nh káº¿t quáº£ trÆ°á»›c vÃ  sau khi train LoRA
Evaluates GPT-2 model before and after LoRA training on E2E NLG task.
"""

import os
import torch
import numpy as np
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from collections import Counter

# Giáº£ sá»­ loralib Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../'))
import loralib as lora


class SimpleGPT2WithLoRA(torch.nn.Module):
    """GPT-2 model with LoRA adaptation layers"""
    
    def __init__(self, pretrained_model='gpt2', lora_rank=16, lora_alpha=32, lora_dropout=0.05):
        super().__init__()
        self.model = GPT2LMHeadModel.from_pretrained(pretrained_model)
        self.lora_rank = lora_rank
        self.lora_alpha = lora_alpha
        
        # Ãp dá»¥ng LoRA vÃ o cÃ¡c c_attn vÃ  c_proj layers
        for name, module in self.model.named_modules():
            if isinstance(module, torch.nn.Linear) and ('c_attn' in name or 'c_proj' in name):
                # Táº¡o LoRA layer
                lora_module = lora.Linear(
                    module.in_features,
                    module.out_features,
                    r=lora_rank,
                    lora_alpha=lora_alpha,
                    lora_dropout=lora_dropout,
                    bias=module.bias is not None
                )
                # Copy weights
                lora_module.weight.data = module.weight.data.clone()
                if module.bias is not None:
                    lora_module.bias.data = module.bias.data.clone()
                
                # Replace module
                parent_name = '.'.join(name.split('.')[:-1])
                module_name = name.split('.')[-1]
                parent = self.model.get_submodule(parent_name)
                setattr(parent, module_name, lora_module)
        
        # Mark only LoRA as trainable
        lora.mark_only_lora_as_trainable(self.model)
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
    
    def generate(self, input_ids, max_length=50, num_beams=1):
        """Generate text using the model"""
        return self.model.generate(
            input_ids=input_ids,
            max_length=max_length,
            num_beams=num_beams,
            early_stopping=True,
            top_p=0.95,
            do_sample=True,
            pad_token_id=self.model.config.eos_token_id
        )


def calculate_perplexity(model, texts, tokenizer, device='cpu'):
    """TÃ­nh Perplexity - Ä‘o lÆ°á»ng Ä‘á»™ "bá»‘i rá»‘i" cá»§a mÃ´ hÃ¬nh"""
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for text in texts:
            inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
            input_ids = inputs['input_ids'].to(device)
            attention_mask = inputs['attention_mask'].to(device)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)
            loss = outputs.loss
            
            if loss is not None:
                total_loss += loss.item() * input_ids.shape[1]
                total_tokens += input_ids.shape[1]
    
    if total_tokens > 0:
        avg_loss = total_loss / total_tokens
        perplexity = torch.exp(torch.tensor(avg_loss))
        return perplexity.item()
    return float('inf')


def calculate_entropy(model, texts, tokenizer, device='cpu'):
    """TÃ­nh Entropy - Ä‘o lÆ°á»ng Ä‘á»™ cháº¯c cháº¯n cá»§a dá»± Ä‘oÃ¡n"""
    model.eval()
    total_entropy = 0
    total_predictions = 0
    
    with torch.no_grad():
        for text in texts:
            inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
            input_ids = inputs['input_ids'].to(device)
            
            outputs = model(input_ids=input_ids, labels=input_ids)
            logits = outputs.logits
            
            # TÃ­nh entropy tá»« logits
            probs = torch.softmax(logits, dim=-1)
            entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1).mean()
            
            total_entropy += entropy.item()
            total_predictions += 1
    
    return total_entropy / total_predictions if total_predictions > 0 else 0


def evaluate_generation_quality(model, prompts, tokenizer, device='cpu', max_length=50):
    """ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng vÄƒn báº£n Ä‘Æ°á»£c táº¡o ra"""
    model.eval()
    results = []
    
    with torch.no_grad():
        for prompt in prompts:
            input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
            
            # Generate output
            output_ids = model.generate(input_ids, max_length=max_length)
            output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
            
            # TÃ­nh metrics
            output_length = len(output_ids[0])
            vocab_diversity = len(set(output_ids[0].tolist()))
            
            results.append({
                'prompt': prompt,
                'output': output_text,
                'output_length': output_length,
                'vocab_diversity': vocab_diversity
            })
    
    return results


def count_parameters(model):
    """Äáº¿m sá»‘ lÆ°á»£ng parameters"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params


def print_evaluation_report(model_name, metrics, generation_results):
    """In bÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡ chi tiáº¿t"""
    print(f"\n{'='*70}")
    print(f"MODEL: {model_name}")
    print(f"{'='*70}")
    
    print("\nğŸ“Š METRICS:")
    print(f"  Perplexity (Ä‘á»™ bá»‘i rá»‘i):        {metrics['perplexity']:.4f}")
    print(f"  Entropy (Ä‘á»™ khÃ´ng cháº¯c):        {metrics['entropy']:.4f}")
    print(f"  Average Output Length:          {metrics['avg_output_length']:.2f}")
    print(f"  Average Vocab Diversity:        {metrics['avg_vocab_diversity']:.2f}")
    
    if 'trainable_params' in metrics:
        print(f"\nğŸ’¾ PARAMETERS:")
        print(f"  Total Parameters:               {metrics['total_params']:,}")
        print(f"  Trainable Parameters:           {metrics['trainable_params']:,}")
        trainable_pct = (metrics['trainable_params'] / metrics['total_params'] * 100) if metrics['total_params'] > 0 else 0
        print(f"  Trainable % of Total:           {trainable_pct:.2f}%")
    
    print("\nâœï¸ GENERATION SAMPLES:")
    for i, result in enumerate(generation_results[:3], 1):
        print(f"\n  Sample {i}:")
        print(f"    Input:   {result['prompt']}")
        print(f"    Output:  {result['output'][:100]}...")
        print(f"    Length:  {result['output_length']} tokens")


def main():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nğŸ”§ Device: {device}")
    
    # Load tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    tokenizer.pad_token = tokenizer.eos_token
    
    # Test data - E2E NLG examples
    test_texts = [
        "name : Aachos , eatType : restaurant , food : Indian , priceRange : moderate , area : city centre , familyFriendly : yes",
        "name : Akane , eatType : restaurant , food : Japanese , priceRange : high , area : riverside , familyFriendly : no",
        "name : Browns Cambridge , eatType : pub , food : English , priceRange : moderate , area : city centre , familyFriendly : yes"
    ]
    
    prompts = [
        "The restaurant serves",
        "In the city centre, you can find",
        "For a family-friendly dinner,"
    ]
    
    print("\n" + "="*70)
    print("E2E NLG MODEL EVALUATION")
    print("="*70)
    
    # ========== Evaluate PRETRAINED Model (khÃ´ng train) ==========
    print("\n\nğŸ”„ Loading PRETRAINED model (no fine-tuning)...")
    pretrained_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)
    pretrained_model.eval()
    
    print("âœ… Evaluating PRETRAINED model...")
    pretrained_perplexity = calculate_perplexity(pretrained_model, test_texts, tokenizer, device)
    pretrained_entropy = calculate_entropy(pretrained_model, test_texts, tokenizer, device)
    pretrained_generation = evaluate_generation_quality(pretrained_model, prompts, tokenizer, device)
    
    pretrained_metrics = {
        'perplexity': pretrained_perplexity,
        'entropy': pretrained_entropy,
        'avg_output_length': np.mean([r['output_length'] for r in pretrained_generation]),
        'avg_vocab_diversity': np.mean([r['vocab_diversity'] for r in pretrained_generation]),
        'total_params': sum(p.numel() for p in pretrained_model.parameters()),
        'trainable_params': 0  # KhÃ´ng cÃ³ trainable params
    }
    
    print_evaluation_report("PRETRAINED GPT-2 (No Fine-tuning)", pretrained_metrics, pretrained_generation)
    
    # ========== Load LoRA-trained Model ==========
    lora_checkpoint_path = os.path.join(os.path.dirname(__file__), 'lora_model', 'pytorch_model.bin')
    
    if os.path.exists(lora_checkpoint_path):
        print("\n\nğŸ”„ Loading LoRA FINE-TUNED model...")
        lora_model = SimpleGPT2WithLoRA(lora_rank=16).to(device)
        
        # Load trained weights
        checkpoint = torch.load(lora_checkpoint_path, map_location=device)
        lora_model.model.load_state_dict(checkpoint, strict=False)
        lora_model.eval()
        
        print("âœ… Evaluating LoRA FINE-TUNED model...")
        lora_perplexity = calculate_perplexity(lora_model.model, test_texts, tokenizer, device)
        lora_entropy = calculate_entropy(lora_model.model, test_texts, tokenizer, device)
        lora_generation = evaluate_generation_quality(lora_model.model, prompts, tokenizer, device)
        
        total_params, trainable_params = count_parameters(lora_model.model)
        
        lora_metrics = {
            'perplexity': lora_perplexity,
            'entropy': lora_entropy,
            'avg_output_length': np.mean([r['output_length'] for r in lora_generation]),
            'avg_vocab_diversity': np.mean([r['vocab_diversity'] for r in lora_generation]),
            'total_params': total_params,
            'trainable_params': trainable_params
        }
        
        print_evaluation_report("LoRA FINE-TUNED GPT-2", lora_metrics, lora_generation)
        
        # ========== So sÃ¡nh káº¿t quáº£ ==========
        print("\n\n" + "="*70)
        print("ğŸ“ˆ COMPARISON: PRETRAINED vs FINE-TUNED")
        print("="*70)
        
        perplexity_improvement = ((pretrained_perplexity - lora_perplexity) / pretrained_perplexity * 100) if pretrained_perplexity > 0 else 0
        entropy_improvement = ((pretrained_entropy - lora_entropy) / pretrained_entropy * 100) if pretrained_entropy > 0 else 0
        vocab_improvement = ((lora_metrics['avg_vocab_diversity'] - pretrained_metrics['avg_vocab_diversity']) / pretrained_metrics['avg_vocab_diversity'] * 100) if pretrained_metrics['avg_vocab_diversity'] > 0 else 0
        
        print(f"\nğŸ“Š Metrics Improvement:")
        print(f"  Perplexity:  {pretrained_perplexity:.4f} â†’ {lora_perplexity:.4f}")
        print(f"    âœ“ Cáº£i thiá»‡n: {perplexity_improvement:+.2f}%")
        
        print(f"\n  Entropy:     {pretrained_entropy:.4f} â†’ {lora_entropy:.4f}")
        print(f"    âœ“ Cáº£i thiá»‡n: {entropy_improvement:+.2f}%")
        
        print(f"\n  Vocab Diversity: {pretrained_metrics['avg_vocab_diversity']:.2f} â†’ {lora_metrics['avg_vocab_diversity']:.2f}")
        print(f"    âœ“ Cáº£i thiá»‡n: {vocab_improvement:+.2f}%")
        
        print(f"\nğŸ’¾ Efficiency:")
        print(f"  Trainable Params: {lora_metrics['trainable_params']:,} / {total_params:,}")
        print(f"  Training Efficiency: Only {(lora_metrics['trainable_params']/total_params*100):.2f}% params updated")
        
        # Káº¿t luáº­n
        print(f"\nğŸ¯ CONCLUSION:")
        if perplexity_improvement > 0:
            print(f"  âœ… Model Ä‘Ã£ há»c Ä‘Æ°á»£c - Perplexity giáº£m {perplexity_improvement:.1f}%")
        else:
            print(f"  âš ï¸ Perplexity tÄƒng {abs(perplexity_improvement):.1f}% (cáº§n train nhiá»u hÆ¡n)")
        
        if lora_metrics['avg_vocab_diversity'] > pretrained_metrics['avg_vocab_diversity']:
            print(f"  âœ… Äá»™ Ä‘a dáº¡ng tá»« vá»±ng tÄƒng {vocab_improvement:.1f}%")
        else:
            print(f"  âš ï¸ Äá»™ Ä‘a dáº¡ng tá»« vá»±ng giáº£m")
        
        print(f"  âœ… LoRA chá»‰ cáº­p nháº­t {lora_metrics['trainable_params']:,} params (1.2% tá»•ng)")
        
    else:
        print(f"\nâŒ LoRA checkpoint not found at: {lora_checkpoint_path}")
        print("   Vui lÃ²ng cháº¡y run_training.py trÆ°á»›c!")
    
    print("\n" + "="*70 + "\n")


if __name__ == '__main__':
    main()
