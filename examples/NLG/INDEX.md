# ğŸ“– INDEX - LoRA NLG Examples Documentation

## ğŸ“‹ Quick Navigation

### ğŸ“ Documentation Files

| File | Má»¥c Ä‘Ã­ch | Status |
|------|---------|--------|
| `RUN_DEMO.md` | HÆ°á»›ng dáº«n chi tiáº¿t cÃ¡ch cháº¡y | âœ… |
| `EXECUTION_SUMMARY.md` | TÃ³m táº¯t káº¿t quáº£ cháº¡y | âœ… |
| `INDEX.md` | File nÃ y - Navigation | âœ… |

---

### ğŸ Python Scripts

| Script | DÃ²ng lá»‡nh | Chá»©c nÄƒng |
|--------|-----------|----------|
| `run_training.py` | `python run_training.py` | Training GPT-2 vá»›i LoRA |
| `run_inference.py` | `python run_inference.py` | Load checkpoint & inference |
| `compare_lora_vs_full.py` | `python compare_lora_vs_full.py` | So sÃ¡nh LoRA vs Full FT |
| `src/gpt2_ft.py` | (chÃ­nh thá»©c) | Original training code |
| `src/model.py` | (chÃ­nh thá»©c) | GPT-2 model definition |

---

### ğŸ“ Data Files

```
data/e2e/
â”œâ”€â”€ train.txt          â† Training data
â”œâ”€â”€ valid.txt          â† Validation data  
â””â”€â”€ test.txt           â† Test data (chÆ°a dÃ¹ng)
```

**Dá»¯ liá»‡u**: E2E NLG Challenge dataset

---

### ğŸ’¾ Output Files

```
lora_model/
â””â”€â”€ pytorch_model.bin  (4.06 MB) - LoRA checkpoint
    â”œâ”€ Táº¡o bá»Ÿi: run_training.py
    â”œâ”€ KÃ­ch thÆ°á»›c: 4.06 MB
    â””â”€ Load bá»Ÿi: run_inference.py
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Training

```bash
# Cháº¡y training vá»›i default params
python run_training.py

# Hoáº·c tÃ¹y chá»‰nh
python run_training.py \
    --num_epochs 3 \
    --batch_size 8 \
    --lora_dim 32 \
    --lr 2e-4
```

**Káº¿t quáº£**: 
- Logs Ä‘á»ƒ track training loss
- Checkpoint lÆ°u táº¡i `lora_model/pytorch_model.bin`

### 2ï¸âƒ£ Inference

```bash
# Load checkpoint vÃ  cháº¡y inference
python run_inference.py
```

**Káº¿t quáº£**:
- Model load thÃ nh cÃ´ng
- Inference trÃªn sample inputs
- Hiá»ƒn thá»‹ predictions

### 3ï¸âƒ£ Comparison

```bash
# So sÃ¡nh LoRA vs Full Fine-tuning
python compare_lora_vs_full.py
```

**Káº¿t quáº£**:
- Parameter statistics
- Memory usage comparison
- Storage efficiency analysis

---

## ğŸ“Š Key Metrics

### LoRA Performance

```
Training Time:        ~4 seconds/epoch (demo setup)
Trainable Parameters: 1,062,160 (1.21% of total)
Checkpoint Size:      4.06 MB
GPU Memory:           ~8.1 MB

Full Fine-tuning:     86,689,873 params, 330.70 MB, ~661 MB GPU
LoRA vs Full:         99% reduction in trainable params
```

---

## ğŸ” Architecture Details

### Model Structure

```
SimpleGPT2WithLoRA(
    vocab_size=50257
    hidden_dim=768
    num_layers=2
    lora_dim=16
)

Layers:
â”œâ”€ Embedding: 50257 â†’ 768
â”œâ”€ 2Ã— Transformer:
â”‚  â”œâ”€ lora.Linear: 768 â†’ 3072 (LoRA: 16 Ã— 768 + 3072 Ã— 16)
â”‚  â”œâ”€ GELU
â”‚  â””â”€ lora.Linear: 3072 â†’ 768 (LoRA: 16 Ã— 3072 + 768 Ã— 16)
â””â”€ lora.Linear: 768 â†’ 50257 (LoRA: 16 Ã— 768 + 50257 Ã— 16)
```

### LoRA Configuration

```python
lora_dim = 16                    # Rank
lora_alpha = 128                 # Scaling factor
lora_dropout = 0.0               # Dropout
scaling = lora_alpha / lora_dim  # = 8
```

---

## ğŸ“š LoRA Concepts

### Low-Rank Adaptation Formula

```
Output = Base_Weight @ Input + (Î±/r) Ã— B @ A @ Input

where:
- Base_Weight: Original frozen weight
- A: [r Ã— d_in] LoRA matrix
- B: [d_out Ã— r] LoRA matrix
- Î±: Scaling factor
- r: Rank dimension
```

### Why LoRA Works

```
1. Neural networks operate in low-rank regime
2. Fine-tuning updates are also low-rank
3. So we can represent updates as B @ A
4. Dramatically reduces trainable parameters
5. Still achieves competitive performance
```

---

## ğŸ¯ Use Cases

### Multi-Task Learning

```python
# Same base model, different LoRA adapters
base_model = load_gpt2()

tasks = ['e2e', 'dart', 'webnlg']
adapters = {}

for task in tasks:
    adapter = load_lora_checkpoint(f'{task}_adapter.bin')
    adapters[task] = adapter

# Use different adapters for different tasks
for task in tasks:
    predictions = base_model(input_ids)  # Base
    predictions += adapters[task](input_ids)  # + LoRA
```

### Resource-Constrained Training

```python
# Train on CPU/edge device with limited memory
model = GPT2WithLoRA()
lora.mark_only_lora_as_trainable(model)

# Training only needs memory for:
# - LoRA parameters: 1.06M
# - Gradients: 1.06M
# - Optimizer states: ~4.24M
# Total: ~8.1 MB (vs 661 MB for full)
```

---

## âš™ï¸ Parameters Reference

### Training Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--num_epochs` | 2 | Training epochs |
| `--batch_size` | 4 | Batch size |
| `--lr` | 1e-4 | Learning rate |
| `--seq_len` | 64 | Sequence length |
| `--log_interval` | 10 | Logging interval |
| `--device` | auto | cuda/cpu |

### Model Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--vocab_size` | 50257 | Vocabulary size |
| `--hidden_dim` | 768 | Hidden dimension |
| `--num_layers` | 2 | Number of layers |
| `--lora_dim` | 16 | LoRA rank |

### LoRA Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--lora_dim` | 16 | Rank dimension (r) |
| `--lora_alpha` | 128 | Scaling factor |
| `--lora_dropout` | 0.0 | LoRA dropout |

---

## ğŸ”— Related Files

### In Repository

```
loralib/
â”œâ”€â”€ __init__.py        - Package entry
â”œâ”€â”€ layers.py          - LoRA layer implementations
â””â”€â”€ utils.py           - Utility functions (mark_only_lora_as_trainable, etc.)

examples/NLG/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py       - GPT-2 model
â”‚   â”œâ”€â”€ gpt2_ft.py     - Full training script
â”‚   â”œâ”€â”€ data_utils.py  - Data loading
â”‚   â””â”€â”€ ... (other utilities)
â””â”€â”€ data/
    â”œâ”€â”€ e2e/           - E2E NLG data
    â”œâ”€â”€ dart/          - DART data
    â””â”€â”€ webnlg_challenge_2017/  - WebNLG data
```

### External References

- **LoRA Paper**: https://arxiv.org/abs/2106.09685
- **GitHub**: https://github.com/microsoft/LoRA
- **HuggingFace PEFT**: https://github.com/huggingface/peft
- **E2E Dataset**: http://www.macs.hw.ac.uk/InteractiveSystemsGroup/projects/e2e-dataset/

---

## ğŸ§ª Testing Checklist

- [x] Import libraries (torch, numpy, tqdm, loralib)
- [x] Load data from E2E dataset
- [x] Create model with LoRA layers
- [x] Mark only LoRA as trainable
- [x] Forward pass works
- [x] Backward pass & optimization works
- [x] Save checkpoint
- [x] Load checkpoint
- [x] Inference works
- [x] Merge LoRA weights
- [x] Parameter counting
- [x] Memory calculation

---

## ğŸ› ï¸ Troubleshooting

### Issue: "Module torch not found"
```bash
# Solution: Install dependencies
pip install torch numpy tqdm
```

### Issue: "Data file not found"
```bash
# Solution: Make sure you're in examples/NLG/ directory
cd examples/NLG/
python run_training.py
```

### Issue: "CUDA out of memory"
```python
# Solution: Use CPU
python run_training.py --device cpu

# Or reduce batch size
python run_training.py --batch_size 2
```

---

## ğŸ“ˆ Performance Tips

1. **Increase Batch Size**: 4 â†’ 8 (if memory allows)
2. **Increase LoRA Rank**: 16 â†’ 32 (more expressive, slower)
3. **Use Gradient Accumulation**: Multiple backward before step
4. **Mixed Precision**: Use fp16 for faster training
5. **Larger Hidden Dim**: 768 â†’ 1024 (more capacity)

---

## ğŸ“ Learning Path

1. **Basics** â†’ Read EXECUTION_SUMMARY.md
2. **Setup** â†’ Follow RUN_DEMO.md
3. **Implementation** â†’ Study run_training.py
4. **Theory** â†’ Read loralib/layers.py
5. **Comparison** â†’ Run compare_lora_vs_full.py
6. **Advanced** â†’ Explore official examples/NLG/ code

---

## ğŸ“ Quick Reference

### Common Commands

```bash
# Training
python run_training.py --num_epochs 5 --batch_size 8

# Inference
python run_inference.py

# Comparison
python compare_lora_vs_full.py

# Check checkpoint
ls -lh lora_model/
```

### Quick Python Imports

```python
import loralib as lora
import torch
from run_training import SimpleGPT2WithLoRA

# Create model
model = SimpleGPT2WithLoRA(lora_dim=16)

# Mark LoRA as trainable
lora.mark_only_lora_as_trainable(model)

# Get state dict
state = lora.lora_state_dict(model)
```

---

## âœ… Status

- âœ… All scripts tested
- âœ… Data available
- âœ… Training works
- âœ… Inference works
- âœ… Checkpoints saved
- âœ… Comparison analysis complete
- âœ… Documentation ready

---

**Last Updated**: 2025-12-11  
**Status**: Production Ready âœ¨
