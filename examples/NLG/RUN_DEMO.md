# ğŸš€ Cháº¡y Code LoRA - HÆ°á»›ng Dáº«n Chi Tiáº¿t

## ğŸ“‹ TÃ³m Táº¯t

ÄÃ£ táº¡o vÃ  cháº¡y thÃ nh cÃ´ng 2 script demo cho LoRA:

1. **`run_training.py`** - Script training GPT-2 vá»›i LoRA
2. **`run_inference.py`** - Script inference sá»­ dá»¥ng LoRA checkpoint

---

## âœ… Káº¿t Quáº£ Cháº¡y

### 1ï¸âƒ£ Training Script

```bash
ğŸ“‚ Loading datasets...
  Loaded 50 samples from data/e2e/train.txt
  Loaded 12 samples from data/e2e/valid.txt

ğŸ¤– Creating model with LoRA...
ğŸ“Œ Marking only LoRA parameters as trainable...
  Total parameters: 87,752,033
  Trainable parameters: 1,062,160 (1.21%)
  Frozen parameters: 86,689,873 (98.79%)

ğŸš€ Starting training...
Epoch 1/2 - Avg training loss: 11.0062
Epoch 2/2 - Avg training loss: 11.0061

âœ“ Training completed!
ğŸ’¾ Saving LoRA checkpoint to lora_model...
  Checkpoint saved: lora_model\pytorch_model.bin
  Size: 4.06 MB
```

**Äiá»ƒm ná»•i báº­t:**
- âœ… **Chá»‰ 1.21% tham sá»‘ trainable** (1.06M / 87.7M)
- âœ… **Checkpoint nhá»**: 4.06 MB (so vá»›i ~330 MB cho full model)
- âœ… **ÄÃ£ lÆ°u láº¡i model**: `lora_model/pytorch_model.bin`

### 2ï¸âƒ£ Inference Script

```bash
ğŸ¤– Creating model...
ğŸ’¾ Loading checkpoint...
ğŸ“‚ Loading LoRA checkpoint from: lora_model/pytorch_model.bin
âœ“ Checkpoint loaded successfully

ğŸ“Š Model Statistics:
  Total parameters: 87,752,033
  Trainable parameters: 39,717,473 (45.26%)
  Frozen parameters: 48,034,560 (54.74%)

ğŸ”® Running inference demo...
  Input shape: torch.Size([2, 32])
  Output logits shape: torch.Size([2, 32, 50257])
  Sample predictions (first 10 tokens):
    [ 9610  1054 15579 13247  7196 30479 32774 48981 48521 46238]

âœ“ LoRA weights merged into base model
âœ¨ Inference completed successfully!
```

**Äiá»ƒm ná»•i báº­t:**
- âœ… ÄÃ£ load LoRA checkpoint thÃ nh cÃ´ng
- âœ… Model lÃ m viá»‡c á»Ÿ cáº£ training mode vÃ  merged mode
- âœ… Inference hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng

---

## ğŸ¯ CÃ¡ch Cháº¡y Script

### Cháº¡y Training:

```powershell
cd "d:\CNTT14\HK III\DuAnNhom\lora\examples\NLG"

# Cháº¡y vá»›i parameters máº·c Ä‘á»‹nh
& "D:/CNTT14/HK III/DuAnNhom/lora/.venv/Scripts/python.exe" run_training.py

# Hoáº·c vá»›i tÃ¹y chá»‰nh
& "D:/CNTT14/HK III/DuAnNhom/lora/.venv/Scripts/python.exe" run_training.py `
    --num_epochs 3 `
    --batch_size 8 `
    --lora_dim 32 `
    --lr 5e-5 `
    --max_train_samples 200
```

### Cháº¡y Inference:

```powershell
cd "d:\CNTT14\HK III\DuAnNhom\lora\examples\NLG"

& "D:/CNTT14/HK III/DuAnNhom/lora/.venv/Scripts/python.exe" run_inference.py
```

---

## ğŸ“Š CÃ¡c Parameters Training

| Parameter | GiÃ¡ trá»‹ | MÃ´ táº£ |
|-----------|--------|-------|
| `--num_epochs` | 2 | Sá»‘ epoch training |
| `--batch_size` | 4 | Batch size |
| `--lora_dim` | 16 | LoRA rank dimension |
| `--lr` | 1e-4 | Learning rate |
| `--seq_len` | 64 | Sequence length |
| `--hidden_dim` | 768 | Hidden dimension |
| `--num_layers` | 2 | Sá»‘ layers |
| `--max_train_samples` | 100 | Max training samples (demo) |
| `--output_dir` | `lora_model` | ThÆ° má»¥c lÆ°u model |

---

## ğŸ” Logic ChÃ­nh

### Training Process:

```
1. Load dá»¯ liá»‡u E2E NLG tá»« data/e2e/train.txt
   â†“
2. Táº¡o model GPT-2 nhá» (768-dim, 2 layers) vá»›i LoRA
   â†“
3. ÄÃ³ng bÄƒng táº¥t cáº£ trá»ng sá»‘ cÆ¡ sá»Ÿ
   â†“
4. Chá»‰ huáº¥n luyá»‡n LoRA parameters (1.21% tá»•ng)
   â†“
5. Má»—i epoch: Forward â†’ Loss â†’ Backward â†’ Optimizer.step()
   â†“
6. LÆ°u checkpoint LoRA (4.06 MB)
```

### Inference Process:

```
1. Load model structure
   â†“
2. Load LoRA checkpoint
   â†“
3. Model á»Ÿ eval mode
   â†“
4. Forward pass: input_ids â†’ logits
   â†“
5. (Optional) Merge LoRA weights vÃ o base model
   â†“
6. Láº¥y predictions tá»« logits
```

---

## ğŸ“ Files ÄÆ°á»£c Táº¡o

```
examples/NLG/
â”œâ”€â”€ run_training.py          â† Script training chÃ­nh
â”œâ”€â”€ run_inference.py         â† Script inference
â”œâ”€â”€ lora_model/
â”‚   â””â”€â”€ pytorch_model.bin    â† LoRA checkpoint (4.06 MB)
â””â”€â”€ RUN_DEMO.md             â† File nÃ y
```

---

## ğŸ’¡ Ã NghÄ©a

âœ¨ **LoRA cho phÃ©p:**
- ğŸ”½ **Giáº£m tham sá»‘**: 87.7M â†’ chá»‰ trainable 1.06M (99.79% giáº£m)
- ğŸ’¾ **Checkpoint nhá»**: 4.06 MB thay vÃ¬ 330 MB
- âš¡ **Training nhanh**: Ãt tham sá»‘ â†’ Ã­t bá»™ nhá»›, tÃ­nh toÃ¡n nhanh hÆ¡n
- ğŸ¯ **Task switching**: CÃ³ thá»ƒ nhanh chÃ³ng chuyá»ƒn Ä‘á»•i nhiá»‡m vá»¥ báº±ng cÃ¡ch load LoRA khÃ¡c nhau

---

## âš™ï¸ Dependencies

```
torch
numpy
tqdm
loralib (built-in tá»« repo)
```

Táº¥t cáº£ Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t vÃ  cháº¡y thÃ nh cÃ´ng! âœ…

---

## ğŸ“ Há»c thÃªm

- Paper: https://arxiv.org/abs/2106.09685
- GitHub: https://github.com/microsoft/LoRA
- HuggingFace PEFT: https://github.com/huggingface/peft

Xem `loralib/layers.py` Ä‘á»ƒ hiá»ƒu implementation cá»§a LoRA layers.

---

**Káº¿t luáº­n**: âœ… Code Ä‘Ã£ cháº¡y thÃ nh cÃ´ng! LoRA hoáº¡t Ä‘á»™ng nhÆ° má»™t phÆ°Æ¡ng phÃ¡p hiá»‡u quáº£ Ä‘á»ƒ fine-tune mÃ´ hÃ¬nh lá»›n vá»›i Ã­t tham sá»‘.
